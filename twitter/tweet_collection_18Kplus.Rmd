---
title: "Collect Tweet Data (more than 18K!)"
author: "Meltem Odabas"
date: "June 8, 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Tweet Collection

In this tutorial I will show you how to collect **more than 18K tweets** and save them to our computer in an **organized, systematic way**.

## Let's start with organizing! Define your data folder location, create folders if needed.

One good Rscript writing practice is to define your main data folder as your 'bucket' from the very beginning. This is ideally a directory that is separate from where you save your R files for coding. Have one folder for codes, another for your dataset. So, you might be working on a completely different directory in R [this is what you see when you type getwd()]; but when you save your data, you will save it to a file or folder located in your bucket, regardless of which directory you are working at easily!

Setting your bucket in your R code will also help you to change the bucket name easily (for instance, if you switch from one computer to another).

Ok, let's set your bucket:

```{r}
bucket <- 'type your bucket location here'
#for me, it is the following, so you will need to change this for your own use:
bucket <- 'D:/Work_20200326/datasets/github-tutorials'
```

So, this is your bucket name. Maybe you already opened up this folder, or maybe you did not. The code below checks if this folder already exists, and if not, creates this bucket for you:

```{r}
if(dir.exists(bucket)==FALSE){
  dir.create(bucket)
}
```

Next is to speficy a new folder in this bucket folder. This way I know the data is coming from this particular RMarkdown file!
```{r}
file.path(bucket, 'twitter', 'tweet_collection_18Kplus') #the file.path function takes all the elements in it and merges them in a way that you can use it as a file or folder location!
tmp <- 'twitter' #the upper folder
export_loc <-  file.path(bucket, 'twitter', 'tweet_collection_18Kplus') #the folder where we will save our tweet data

#check whether the twitter folder exists first, (i.e., the upper folder)
if(dir.exists(file.path(bucket,tmp))==FALSE){
  dir.create(file.path(bucket,tmp))
}
rm(tmp)
#and then create the folder under the upper folder -- this is going to be our "export location"
if(dir.exists(file.path(export_loc))==FALSE){
  dir.create(file.path(export_loc))
}
```

Great! so now we have a new folder for saving the data we will collect.

##Let's practice tweet data collection first

Ok, so before we think too much about how to save the files, let's learn how to collect tweets first! I am assumng you will use your personal standard twitter API. I will not got into the details of how to get a Twitter Developer API, I am pretty sure search engines are full of such kind of instructions. 

Since we will use the **rtweet** package for tweet collection, we will set our token first, following the code below. Keep in mind: never share your tokens or passwords with others. Therefore you will not see mine written here, but keep in mind that I am using that information in the background:

```{r, eval=F, echo=T}
## load rtweet package and other packages you need
library(rtweet)
library(stringr)

##
#fill in the information to the lines 7-11 from the Twitter developer website

appname = 'your-ap-name-here'
key = 'your-key-here'
secret = 'your-secret-here'
access_token = 'your-access-token-here'
access_secret= 'your-access-secret-here'

twitter_token <- create_token(
  app = appname,
  consumer_key = key,
  consumer_secret = secret,
  access_token = access_token,
  access_secret = access_secret)

```

```{r include=FALSE}
## load rtweet package and other packages you need
library(rtweet)
library(stringr)

#I saved my keys to a csv file and will upload the keys from there. Good practice to not share your keys publicly while writing a tutorial! :)

keys = read.csv("E:/data/protest202101/key/meltem_key.csv",
                           header = T, stringsAsFactors = FALSE)
for (i in 1:nrow(keys)){
  assign(keys$name[i], keys$value[i])
}

twitter_token <- create_token(
  app = 'meltemsapp',
  consumer_key = api_key,
  consumer_secret = api_secret_key,
  access_token = access_token,
  access_secret = access_token_secret)
```

There are different ways of collecting tweets with rtweet package using the standard Twitter API:

* **stream_tweets().** Stream in real time (i.e., live stream). This does not have a rate limit. This search is known to give approx. 10% of the all tweets relevant to your search.
* **search_tweets().** Collect the most recent tweets. There is a rate limit, meaning that you can collect at most 18K tweets within 15 minutes. This method allows you to go back in time while collecting, however no more than 6-9 days. So, keep that in mind. 
* **get_timeline().**Select a user and collect their most recent tweets (you can collect up to 3.2K tweets. No more)

For more options, *check this website*[https://rtweet.info/articles/intro.html]

**EDIT: I wrote this code before AcademicAPI was released. Please check the limits for AcademicAPI if you have one, and also please take a look at the AcademicTwitteR package[https://github.com/cjbarrie/academictwitteR]**

we will use **search_tweets()** today.

Let's search for a hashtag! How about... '#rladies' ? And for our practice let's collect 5 tweets:
```{r}
query <- "#rladies"
collect_count <- 5

tweets <- search_tweets(query,
                        include_rts=FALSE, #decide whether you want to include retweets or not.
                        n=collect_count)
```

This dataset should include many columns. Let's check its dimensions and the names of the columns to see what kind of information we are collectiong regarding these tweets (and the users posting these tweets!)

```{r}
dim(tweets)
names(tweets)
tweets[1,1:10] #list the first 10 columns
```

This is a lot of information! Right now we have 5 tweets, but what if we were to have... A million tweets?!? Well, there are ways to save to data frames in compressed formats. For example, you can save it as a csv, fst, or a compressed csv file. For bigger files I prefer to use the data.table package for working with large files and save them as fst, so this is what I will use for now. However, you can use other options!

**Note:** There is also an option to save the data as json file, but this is not what I will go into in today's tutorial.

```{r, echo=FALSE, results="hide"}
library(data.table)
library(fst)
```

```{r}
fname_test <- file.path(export_loc,"test.fst")
fname_test
#save the file in fst format
fwrite(tweets, fname_test)
```

But, let's delete this for now, so that we can start from scratch.

```{r}
file.remove(fname_test)
```

##Now it is time for the real deal: collect more than 18K!

So, there is this thing called 'rate limit,' and you need to be beware about this. When using Twitter's standard API to collect data from the past, you are limited with collection 18K tweets at a time. This means, you need to make sure that you need to wait for 15 minute before you ask for the 18,001st tweet you request to collect. Although **search_tweets()** function includes a parameter called *retryonratelimit = * and when you set this to *"TRUE"* it will to the sleeping part automatically for you, if you are aiming to collect, say, 1 million tweets, there is a huge chance that your connection might be lost, and you can end up losing what you collected thus far. Therefore, my suggestion is to write your code so that even if you lose connection or your function breaks for some unforeseen reason, you still get to keep the data you collected thus far, and you can get to save that.

To this is what we will focus on today.

###Remind your future self what you searched for n your query

search_tweets() function allows you to do multiple term searches at a time. For example, it is possible to not only search for #rladies" but also "#rstats" at one go. You will know what your query included today, but, when you look at your data, sat, three months from now, will you remember what exactly you searched for? So, create a reminder file for yourself. A very simple text file.

```{r}
q <- list()
q[[1]] <- paste("search query:",query)
fwrite(q, file = file.path(export_loc,"readme.txt"))
```

So now, we can one file in our folder, named as "readme.txt"
```{r}
files <- list.files(path = export_loc)
files
```

Great. Now,this is what we will want to do. We will want to collect the data in chunks. Say 18K at a time.At every round, we will store the data collected in a variable. Once we reach the rate limit, we will ask our code to not to do anything (i.e., sleep) for 15 minutes. Then, we will want to save a file once we reach to, say, 180K tweets. This way, we will save a big enough list of tweets in a file, but the file will not be a giant one that will slow down opening process for R when we need to use it in the future. And we will do rounds of that until we reach the total number of tweets we want to collect (say, 1M).

Let's start with setting up those numbers

```{r}
n_search <- 18000
n_total <- 1000000
round_total <- 10 #we will make 10 rounds of 18K tweet collection and then save as we reach to 180K collected tweets
```

We can write a loop function to do this for us. Before we do so, however, keep in mind that search_tweets() function collects tweets in the order from newest to the oldest. This means that if you do not set your parameters right, you will end up collecting the same set of tweets in your following searches, and not be able to go back in time while collecting your data.

The parameter you need to be aware of for this purpose is called *max_id()*. Say, in your first search, you collected five tweets with the following tweet numbers: 3046,2034,1984,832,824 (because the newest tweets will have higher numbers, this is how they will be ordered. The newest tweet is collected first, then an older one, and so forth). In your second search, if you set your max_id parameter to max_id="824", this will search tweets with the id number 824 or lower that fit your search criteria. This way, you will know for sure that you are collecting older tweets from what you have collected thus far.

Let's start with writing a function that collects 180K tweets:
```{r, eval=T, echo=T}
#first, set the parameters
# query <- "#rladies"
# n_search <- 18000
# n_total <- 1000000
# round_total <- 10
# rt <- FALSE #do not include retweets in our search
# maxid <- NULL #for the first round, set maxid to null, which will let us collect the newest tweets.

#FUNCTION:
collect180K = function(query,
                       n_search = 18000,
                       round_total = 10,
                       rt = FALSE,
                       maxid = NULL){
  #create an empty list, we will save the 10 dataframes we will collect as list items here.
  tweetList <- list() 
  #the loop:
  for (i in c(1:round_total)){
    print(paste("data number:" ,i, "out of", round_total))
    tweets <- search_tweets(query,
                            include_rts=rt, 
                            n=n_search,
                            max_id = maxid)
    tweetList[[i]] <- tweets
    maxid <- tail(tweets,1)$status_id #reset the maxid to use it in the next round of tweet collection
    rm(tweets)
    #sleep for 15 minutes before collecting the next dataset
    if(i<round_total){
      print('sleeping for 15 minutes')
      Sys.sleep(60*15)
      }
    }
  result <- rbindlist(tweetList) #bind all the dataframes in the list to one dataframe. a data.table function.
  result
  }


```

We also need to be careful about naming our files. Each data should have a number, such as "tweets_1.fst","tweets_2.fst" and so on. Note that search_tweets() function collects tweets in the order from newest to the oldest, so the file with the lowest number value will include the newest tweets. Also, keep in mind that you may reach to two-digit or three-digit file numbers if you are collecting a massive dataset. To make sure that the datasets will be ordered from newest to the oldest in your folder, add preceding zeros to those numbers, such as "tweets_001.fst","tweets_002.fst"

```{r}
paste0("tweets_",sprintf("%03d", 1),".fst")
paste0("tweets_",sprintf("%03d", 54),".fst")
```

Now, if you are starting to collect tweets fresh, you will give your file the name "tweets_001.fst". However, it is possible that your code might break and you might need to start over. You do not want to overwrite what you collected thus far, right?

Say, your last saved file is names as "tweets_005.fst". You want to make sure of two things in this case:

*Naming your next file as "tweets_006.fst",
*Setting the max_is to the status_id value of the last row of the dataset saved as "tweets_005.fst".

So, you want your code to do this:

*check if there is only one (i.e., readme.txt file) saved in the folder.
*if so, 
  +set your starting maxid to NULL , collect your data, and save it as  "tweets_001.fst".
*otherwise,
  + Otherwise, check the name of the last file listed in your folder, grab the digits from the filename, and add 1 to that value.
  +read the status_id column of the last saved dataset, and set your maxid parameter to the value listed in the last row.
  
So, let's revise our former loop to collect 180K tweets and then save it to its designated folder:

```{r, eval=T, echo=T}
#FUNCTION:
collect180K = function(query,
                       n_search = 18000,
                       round_total = 10,
                       rt = FALSE,
                       maxid = NULL,
                       export_to #set the folder you will save the files to.
                       ){
  #create an empty list, we will save the 10 dataframes we will collect as list items here.
  tweetList <- list() 
  #read the list of files saved under the data folder to a variable called 'files'
  files <- list.files(path = export_loc)
  
  #if there are files already saved in the folder other than the readme file:
  if (length(files)>1){
    tmp <- fread(file.path(export_loc,files[length(files)]), select = "status_id")
    #get the status_id of the oldest tweet, set it as `maxid` (i.e. update maxid)
    maxid <- as.character(tail(tmp$status_id, 1))
    #set the file number based on the number of the last saved file (add one)
    file.no <- as.numeric(
      str_extract(files[length(files)], '[[:digit:]]+')
      ) + 1
    rm(tmp) #remove tmp
  }else{
    file.no <- 1 #and maxid is still null
  }

  #the loop:
  for (i in c(1:round_total)){
    print(paste("file number:",file.no,"; data number:",i,"out of",round_total))
    tweets <- search_tweets(query,
                            include_rts=rt, 
                            n=n_search,
                            max_id = maxid)
    tweetList[[i]] <- tweets
    maxid <- tail(tweets,1)$status_id #reset the maxid to use it in the next round of tweet collection
    rm(tweets)
  #sleep for 15 minutes before collecting the next dataset
    if(i<round_total){
      print('sleeping for 15 minutes')
      Sys.sleep(60*15)
    }
  }
  result <- rbindlist(tweetList)
  #save the dataset
  fwrite(result, file = file.path(export_to,paste0("tweets_",sprintf("%03d", file.no),".fst")))
}

```

But what if the total number of tweets we are searching is less than 180K? We need the loop to break at that point. To do so, we can create a count variable:

```{r, eval=T, echo=T}
#FUNCTION:
collect180K = function(query,
                       n_total,
                       export_to, #set the folder you will save the files to.
                       n_search = 18000,
                       round_total = 10,
                       rt = FALSE,
                       maxid = NULL,
                       count = 0
                       ){

  #create an empty list, we will save the 10 dataframes we will collect as list items here.
  tweetList <- list() 
  #read the list of files saved under the data folder to a variable called 'files'
  files <- list.files(path = export_loc)
  
  #if there are files already saved in the folder other than the readme file:
  if (length(files)>1){
    tmp <- fread(file.path(export_loc,files[length(files)]), select = "status_id")
    #get the status_id of the oldest tweet, set it as `maxid` (i.e. update maxid)
    maxid <- as.character(tail(tmp$status_id, 1))
    #set the file number based on the number of the last saved file (add one)
    file.no <- as.numeric(
      str_extract(files[length(files)], '[[:digit:]]+')
      ) + 1
    rm(tmp) #remove tmp
  }else{
    file.no <- 1 #and maxid is still null
  }

  
  #the loop:
  #run the loop only if count is smaller than the total number of tweets we want to collect:
  if(count < n_total){
    for (i in c(1:round_total)){
      print(paste("file number:",file.no,"; data number:",i,"out of",round_total))
      tweets <- search_tweets(query,
                            include_rts=rt, 
                            n=n_search,
                            max_id = maxid)
      tweetList[[i]] <- tweets
      maxid <- tail(tweets,1)$status_id #reset the maxid to use it in the next round of tweet collection
      rm(tweets)
      #update count
      count = count + n_search
      #sleep for 15 minutes before collecting the next dataset
      if(i<round_total & count<n_total){
        print('sleeping for 15 minutes')
        Sys.sleep(60*15)
      }
    }
  }

  result <- rbindlist(tweetList)
  #save the dataset
  fwrite(result, file = file.path(export_to,paste0("tweets_",sprintf("%03d", file.no),".fst")))
}

```

  
Finally, since we want to collect and save sets of 180K tweets until we reach the total of 1M, all we need to do is to use the same function (collect180K) but automatically adjust the *round_total* parameter. 

```{r, eval=T, echo=T}

collectNsave = function(query,
                       n_search = 18000,
                       round_total = 10,
                       n_total = 10^6,
                       rt = FALSE,
                       maxid = NULL,
                       export_to,
                       count=0){
  #update round_total number
  round_total = ceiling(n_total/n_search)
  print(round_total)
  collect180K(query=query,
              n_total=n_total,
              n_search=n_search, 
              round_total=round_total,
              rt=rt,
              maxid=maxid,
              export_to=export_to)
  print(paste(count, 'tweets collected and saved with this search in total.'))
}

```

Because collection 1M tweets this would take a long time to collect, I will only test for 30K tweets.

```{r, eval=T, echo=T}
collectNsave(query = '#coronavirus',
             n_total = 3*10^4,
             export_to = export_loc)
```

Now, let's check the list of files in our folder:
```{r}
files <- list.files(path = export_loc)
files
```

It worked!!! Let's check the total number of rows in the saved dataset before we finish up:

```{r}
df = fread(file.path(export_loc,'tweets_001.fst'))
nrow(df)
```

It actually collected 36K tweets, not 30K, because we did not set up a cutoff for n_search parameter for cases where the number of tweets requested is not a multiplication of the n_search=18000. So, to conclude, I will adjust the code for *collect180K* functions below to correct for that. Instead of count, I will use a new parameter: 'countdown'

```{r}
collect180K = function(query,
                       n_total,
                       export_to, #set the folder you will save the files to.
                       n_search = 18000,
                       round_total = 10,
                       rt = FALSE,
                       maxid = NULL
                       ){
  countdown = n_total
  
  #create an empty list, we will save the 10 dataframes we will collect as list items here.
  tweetList <- list() 
  #read the list of files saved under the data folder to a variable called 'files'
  files <- list.files(path = export_loc)
  
  #if there are files already saved in the folder other than the readme file:
  if (length(files)>1){
    tmp <- fread(file.path(export_loc,files[length(files)]), select = "status_id")
    #get the status_id of the oldest tweet, set it as `maxid` (i.e. update maxid)
    maxid <- as.character(tail(tmp$status_id, 1))
    #set the file number based on the number of the last saved file (add one)
    file.no <- as.numeric(
      str_extract(files[length(files)], '[[:digit:]]+')
      ) + 1
    rm(tmp) #remove tmp
  }else{
    file.no <- 1 #and maxid is still null
  }

  
  #the loop:
  #run the loop only if count is smaller than the total number of tweets we want to collect:
  if(countdown > 0){
    
    if(countdown < n_search){n_search = countdown} #update n_search based on remaining number of tweets to be collected
    
    for (i in c(1:round_total)){
      print(paste("file number:",file.no,"; data number:",i,"out of",round_total))
      tweets <- search_tweets(query,
                            include_rts=rt, 
                            n=n_search,
                            max_id = maxid)
      tweetList[[i]] <- tweets
      maxid <- tail(tweets,1)$status_id #reset the maxid to use it in the next round of tweet collection
      rm(tweets)
      #update countdown
      countdown = countdown - n_search
      #sleep for 15 minutes before collecting the next dataset
      if(i<round_total & countdown>0){
        print('sleeping for 15 minutes')
        Sys.sleep(60*15)
      }
    }
  }
  result <- rbindlist(tweetList)
  #save the dataset
  fwrite(result, file = file.path(export_to,paste0("tweets_",sprintf("%03d", file.no),".fst")))
}
```

Now, let's try again. This way, we can also test whether the new dataset will contain older tweets (i.e. whether the function will be able to update the maxid parameter successfully)

```{r, eval=T, echo=T}
collectNsave(query = '#coronavirus',
             n_total = 3*10^4,
             export_to = export_loc)
```

This time we should have two datasets.
```{r}
files <- list.files(path = export_loc)
files
```

Let check the total number of tweets in the second dataset

```{r}
df2 = fread(file.path(export_loc,'tweets_002.fst'))
nrow(df2)
```

And whether the second dataset collected older tweets
```{r}
#print IDs of the first and last tweet in df1
print(df[1,1],df[nrow(df),1])

#print IDs of the first and last tweet in df2
print(df2[1,1],df2[nrow(df),1])
```
